{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classification lightgbm with parameter tuning\n## Optuna will be used for hyper parameter optimization\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport sklearn.metrics\nimport datatable as dt\n!pip install --quiet optuna\nimport optuna\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reducing the data frame by choosing appropraite data types depending on the type of data not only decreases the modelling time also makes the use of computational resources appropriately.\n[https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using datatable improves the dataload part, it also can be used directly without any conversions."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(dt.fread('../input/jane-street-market-prediction/train.csv').to_pandas())\nfeatures = reduce_mem_usage(dt.fread('../input/jane-street-market-prediction/features.csv').to_pandas())\ntest = reduce_mem_usage(dt.fread('/kaggle/input/jane-street-market-prediction/example_test.csv').to_pandas())\ny=np.where(train['resp'] > 0,1,0)\ntrain= train.loc[:, ~train.columns.str.startswith('res')] #train[features['feature']]\ntrain= train.loc[:, ~train.columns.str.startswith('ts_id')]\ntest= test.loc[:, ~test.columns.str.startswith('res')] #train[features['feature']]\ntest= test.loc[:, ~test.columns.str.startswith('ts_id')]\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(train)\nX_test = scaler.transform(test)\ntrain_x, valid_x, train_y, valid_y = train_test_split(X_train, y, test_size=0.25)\ndtrain = lgb.Dataset(train_x, label=train_y)\ndvalid = lgb.Dataset(valid_x, label=valid_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[features['feature']].isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyper Parameters to be tuned\nmetric: different metric types like auc.<br><br>\nboosting type: there are 4 different values:<br><br>\n                default: gbdt - Stable reliable but time consuming.<br><br>\n                dart: better accuracy but requires hyper parameters to be tuned.<br><br>\n                goss: converges faster but may overfit.<br><br>\nmax_bin: choose smaller value to avoid overfit<br><br>\nnum_leaves: choose smaller value to avoid overfit<br><br>\nbagging_fraction and bagging_freq:  example, {\"bagging_freq\": 5, \"bagging_fraction\": 0.75} tells LightGBM “re-sample without replacement every 5 iterations, and draw samples of 75% of the training data”<br><br>\nfeature_fraction: value between 0-1, tells what fraction of features have to be considered.<br><br>\nmin_gain_to_split: Gain is basically the reduction in training loss that results from adding a split point. Default value is 0, can increase this value to increase spees also for regularization. <br><br>\nmax_depth: keep it as low as possible to avoid overfitting.<br><br>\nlambda1(l1) and lambda2(l2): helps in regularizing and to avoid overfitting.<br><br>\nnum_iterations: Start with a low number for base model and increase to increase efficiency.<br><br>\nearly_stopping_rounds: Ideally should be 10% of num_iterations.<br><br>\ncategorical_feature: can provide a list which has categorical values<br><br>\nis_unbalance: if true will do balancing autoatically<br><br>\nsample_pos_weight : value between 0-1.number of negative samples / number of positive samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass Objective:\n\n    def __init__(self):\n        self.best_booster = None\n        self._booster = None\n\n    def __call__(self, trial):\n        param = {\n            \"objective\": \"binary\",\n            \"metric\": \"auc\",\n            \"verbosity\": -1,\n            \"boosting_type\": \"gbdt\",\n            \"bagging_fraction\": trial.suggest_loguniform(\"bagging_fraction\", 0.3, 1.0),\n            \"feature_fraction\": trial.suggest_loguniform(\"feature_fraction\", 0.3, 1.0),\n            \"min_gain_to_split\": trial.suggest_loguniform(\"min_gain_to_split\", 0.3, 1.0),\n            \"max_depth\": -1,\n            \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n            \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 300),\n            \"num_iterations\": trial.suggest_int(\"num_iterations\", 200, 1000),\n            \"early_stopping_rounds\": trial.suggest_int(\"early_stopping_rounds\", 20, 100),\n            \"objective\":\"binary\",\n            \"learning_rate\":trial.suggest_loguniform(\"learning_rate\", 0.01, 0.1),\n            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 1000)\n        }\n\n        # Add a callback for pruning.\n        pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\")\n        gbm = lgb.train(\n            param, dtrain, valid_sets=[dvalid], verbose_eval=False, callbacks=[pruning_callback]\n        )\n\n        self._booster = gbm\n\n        preds = gbm.predict(valid_x)\n        pred_labels = np.rint(preds)\n        accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n        return accuracy\n\n    def callback(self, study, trial):\n        if study.best_trial == trial:\n            self.best_booster = self._booster","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"objective = Objective()\n\nstudy = optuna.create_study(\n    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\"\n)\nstudy.optimize(objective, n_trials=10, callbacks=[objective.callback])\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\nbest_model = objective.best_booster\n\nnew_preds = best_model.predict(X_train)\nnew_pred_labels = np.rint(new_preds)\nprint(sklearn.metrics.accuracy_score(y, new_pred_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import lightgbm as lgb\n# from lightgbm import LGBMClassifier\n# from sklearn.model_selection import train_test_split\n\n# xtr,xval,ytr,yval = train_test_split(X_train ,y,test_size=0.30,stratify=y)\n# lg = lgb.LGBMClassifier(eval_metric='auc',metric='auc', \n#                         bagging_fraction= 0.7074669591958046,\n#     feature_fraction= 0.42131413818337216,\n#     min_gain_to_split= 0.3812146181153679,\n#     lambda_l1= 0.00167686294813971,\n#     lambda_l2= 1.3583979451485277,\n#     num_leaves= 189,\n#     num_iterations= 569,\n#     learning_rate= 0.09196718727893016,\n#     n_estimators= 214,\n#                         objective='binary',#learning_rate=0.05, \n#                         silent=False,\n#                         force_col_wise=True)\n# lg.fit(xtr,ytr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_pred=lg.predict(xtr)\n# y_val1=lg.predict(xval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.metrics import f1_score,accuracy_score,classification_report\n# print(f1_score(ytr, y_pred, average='macro'))\n# print(accuracy_score(ytr, y_pred))\n# print(classification_report(ytr, y_pred))\n# print(\"********************************************\")\n# print(f1_score(yval, y_val1, average='macro'))\n# print(accuracy_score(yval, y_val1))\n# print(classification_report(yval, y_val1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import lightgbm as lgbm\n# from lightgbm import LGBMClassifier\n# from sklearn.model_selection import learning_curve, GridSearchCV\n# from sklearn.model_selection import train_test_split\n# params={\"num_leaves\":[100,150,200,300],\n#        \"max_bin\":[30,50,7],\n# #        \"feature_fraction\":0.52,\n# #        \"bagging_fraction\":0.52,\n#        \"objective\":[\"binary\"],\n#        \"learning_rate\":[0.01,0.05,0.1],\n#        \"boosting_type\":[\"gbdt\"],\n#        \"metric\":[\"auc\"],\n#         \"n_estimators\": [200]\n#        }\n# models = [] # list of model , we will train \n# xtr,xval,ytr,yval = train_test_split(X_train ,y,test_size=0.25,stratify=y)\n\n# lg = lgb.LGBMClassifier(silent=False)\n# grid_search = GridSearchCV(lg, n_jobs=2, param_grid=params, cv = 3, scoring=\"roc_auc\", verbose=5)\n# grid_search.fit(xtr,ytr)\n# grid_search.best_estimator_\n\n# # d_train = lgbm.Dataset(xtr,label=ytr)\n# # d_eval = lgbm.Dataset(xval,label=yval,reference=d_train)\n# # clf = lgbm.train(params,d_train,valid_sets=[d_train,d_eval],num_boost_round=2000,\\\n# #                 early_stopping_rounds=100,verbose_eval=100)\n# # models.append(clf)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}